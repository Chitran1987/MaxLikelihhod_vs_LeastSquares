for (k in X) {
Y <- c(f(c(k, c(4, 7, 81))), Y)
}
Y <- NULL
for (k in X) {
Y <- c(f(c(k, c(4, 7, 81))), Y)
}
plot(X,Y)
ClearPlot()
plot(X,Y, type = 'b', pch=19, col=rgb(0,0,1,0.1))
plot(X,Y, type = 'l', pch=19, col=rgb(0,0,1,0.1))
plot(X,Y, type = 'l', pch=19, col=rgb(0,0,1,1))
ClearPlot()
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_norm_sd_81.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- gauss(e, sig =  v[4], mu = 0, probability = T) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
log_LE <- sum(log(G)) #log likelihood term
return(log_LE)
}#The objective function for the optimization
L_LLE <- optim(c(1, 2, 3, 10), f, control = list(fnscale = -1)) #the optimization step
val_LLE <- L_LLE$par
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_norm_sd_27.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
#log-likelihood estimation
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- gauss(e, sig =  v[4], mu = 0, probability = T) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
log_LE <- sum(log(G)) #log likelihood term
return(log_LE)
}#The objective function for the optimization
L_LLE <- optim(c(1, 2, 3, 10), f, control = list(fnscale = -1)) #the optimization step
val_LLE <- L_LLE$par
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_norm_sd_09.txt', sep = '\t', header = T)
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_norm_sd_9.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
#least squares estimation
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
#log-likelihood estimation
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- gauss(e, sig =  v[4], mu = 0, probability = T) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
log_LE <- sum(log(G)) #log likelihood term
return(log_LE)
}#The objective function for the optimization
L_LLE <- optim(c(1, 2, 3, 10), f, control = list(fnscale = -1)) #the optimization step
val_LLE <- L_LLE$par
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_norm_sd_3.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
#least squares estimation
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
#log-likelihood estimation
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- gauss(e, sig =  v[4], mu = 0, probability = T) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
log_LE <- sum(log(G)) #log likelihood term
return(log_LE)
}#The objective function for the optimization
L_LLE <- optim(c(1, 2, 3, 10), f, control = list(fnscale = -1)) #the optimization step
val_LLE <- L_LLE$par
rm(list=ls())
library(StatsChitran)
ClearPlot()
help("runif")
rm(list=ls())
####write a program for a dataset with squared values and noise sampled from a uniform distribution for testing with optimization
#write a program to build a dataset with added noise of a = 0.01
library('StatsChitran') #package available on github
a <- 0.01
X <- seq(-10, 10, by=0.01)
Y <- 3*X^2 +4*X + 7 #underlying structure
Y <- Y + runif(length(X), min = -a*(max(Y) - min(Y)), max = a*(max(Y) - min(Y)))
ClearPlot() #function available with StatsChitran
plot(X,Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
rm(list=ls())
ClearPlot() #function available with StatsChitran
rm(list=ls())
#write a program to build a dataset with added noise of a = 0.01
library('StatsChitran') #package available on github
gen_dat <- function(noise_rat){
noise_rat <- 0.01 #define
X <- seq(-10, 10, by=0.01)
Y <- 3*X^2 +4*X + 7 #underlying structure
Y <- Y + runif(length(X), min = -noise_rat*(max(Y) - min(Y)), max = noise_rat*(max(Y) - min(Y)))
return(data.frame(X,Y))
}
df <- gen_dat(0.01)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
ClearPlot() #function available with StatsChitran
subplot(c(2,2))
df <- gen_dat(0.01)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.05)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.1)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.5)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
rm(list=ls())
#write a program to build a dataset with added noise of a = 0.01
library('StatsChitran') #package available on github
gen_dat <- function(noise_rat){
#noise_rat <- 0.01 #define
X <- seq(-10, 10, by=0.01)
Y <- 3*X^2 +4*X + 7 #underlying structure
Y <- Y + runif(length(X), min = -noise_rat*(max(Y) - min(Y)), max = noise_rat*(max(Y) - min(Y)))
return(data.frame(X,Y))
}
ClearPlot() #function available with StatsChitran
subplot(c(2,2))
df <- gen_dat(0.01)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.05)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.1)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.5)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
rm(list=ls())
####write a program for a dataset with squared values and noise sampled from a uniform distribution for testing with optimization
#write a program to build a dataset with added noise of a = 0.01
library('StatsChitran') #package available on github
gen_dat <- function(noise_rat){
#noise_rat <- 0.01 #define
X <- seq(-10, 10, by=0.01)
Y <- 3*X^2 +4*X + 7 #underlying structure
Y <- Y + runif(length(X), min = -noise_rat*(max(Y) - min(Y)), max = noise_rat*(max(Y) - min(Y)))
return(data.frame(X,Y))
}
ClearPlot() #function available with StatsChitran
subplot(c(2,2))
df <- gen_dat(0.01)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.05)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.1)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.15)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
subplot(c(1,1))
rm(list=ls())
####write a program for a dataset with squared values and noise sampled from a uniform distribution for testing with optimization
#write a program to build a dataset with added noise of a = 0.01
library('StatsChitran') #package available on github
gen_dat <- function(noise_rat){
#noise_rat <- 0.01 #define
X <- seq(-10, 10, by=0.01)
Y <- 3*X^2 +4*X + 7 #underlying structure
Y <- Y + runif(length(X), min = -noise_rat*(max(Y) - min(Y)), max = noise_rat*(max(Y) - min(Y)))
return(data.frame(X,Y))
}
ClearPlot() #function available with StatsChitran
subplot(c(2,2))
df <- gen_dat(0.01)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.05)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.1)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.15)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
subplot(c(1,1))
View(df)
df <- gen_dat(0.01)
write.table(df, file = 'D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.01.txt', sep='\t', col.names = T, row.names = F)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.05)
df <- gen_dat(0.05)
write.table(df, file = 'D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.05.txt', sep='\t', col.names = T, row.names = F)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.1)
write.table(df, file = 'D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.1.txt', sep='\t', col.names = T, row.names = F)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
df <- gen_dat(0.15)
write.table(df, file = 'D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.15.txt', sep='\t', col.names = T, row.names = F)
plot(df$X,df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
rm(list=ls())
library('StatsChitran')
rm(list=ls())
library('StatsChitran')
##read the data
df_u_0.01 <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.01.txt', sep='\t', header = T)
df_u_0.05 <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.05.txt', sep='\t', header = T)
df_u_0.10 <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.10.txt', sep='\t', header = T)
rm(list=ls())
library('StatsChitran')
##read the data
df_u_0.01 <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.01.txt', sep='\t', header = T)
df_u_0.05 <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.05.txt', sep='\t', header = T)
df_u_0.10 <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.1.txt', sep='\t', header = T)
df_u_0.15 <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.15.txt', sep='\t', header = T)
##plot the data
subplot(c(2,2))
plot(df_u_0.01$X, df_u_0.01$Y, col=rgb(0,0,1,0.1), type = 'b', pch=19)
plot(df_u_0.05$X, df_u_0.05$Y, col=rgb(0,0,1,0.1), type = 'b', pch=19)
plot(df_u_0.10$X, df_u_0.10$Y, col=rgb(0,0,1,0.1), type = 'b', pch=19)
plot(df_u_0.15$X, df_u_0.15$Y, col=rgb(0,0,1,0.1), type = 'b', pch=19)
subplot(c(1,1))
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.01.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.05.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
#least squares estimation
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.15.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
#least squares estimation
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- gauss(e, sig =  v[4], mu = 0, probability = T) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
log_LE <- sum(log(G)) #log likelihood term
return(log_LE)
}#The objective function for the optimization
L_LLE <- optim(c(1, 2, 3, 10), f, control = list(fnscale = -1)) #the optimization step
val_LLE <- L_LLE$par
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_norm_sd_3.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
#least squares estimation
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
#log-likelihood estimation
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- gauss(e, sig =  v[4], mu = v[5], probability = T) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
log_LE <- sum(log(G)) #log likelihood term
return(log_LE)
}#The objective function for the optimization
L_LLE <- optim(c(1, 2, 3, 10, 0), f, control = list(fnscale = -1)) #the optimization step
val_LLE <- L_LLE$par
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_norm_sd_3.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
#least squares estimation
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
#log-likelihood estimation
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- gauss(e, sig =  v[4], mu = 0, probability = T) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
log_LE <- sum(log(G)) #log likelihood term
return(log_LE)
}#The objective function for the optimization
L_LLE <- optim(c(1, 2, 3, 10), f, control = list(fnscale = -1)) #the optimization step
val_LLE <- L_LLE$par
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.15.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
#least squares estimation
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- unidistr(df$X, v[4], v[5]) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
help("constrOptim")
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.15.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
##least squares estimation###########################################################
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
unidistr <- function(X, a, b){
if(a >= b){
stop('a should be less than b')
}else{
Y <- rep(NA, length(X))
df <- data.frame(X,Y)
names(df) <- c('X', 'Y')
df$Y[df$X < a] <- 0
df$Y[df$X > b] <- 0
df$Y[is.na(df$Y) == T] <- 1/(b - a)
}
return(df)
}
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- unidistr(df$X, v[4], v[5]) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
LK <- prod(G) #log likelihood term
return(LK)
}#The o
m <- t(c(0, 0, 0, -1, 1))
b <- rep(0, 5)
LK <- constrOptim(c(1, 2, 3, 4, 10), f, control = list(fnscale = -1), ui = m, ci = b ) #the optimization step
##Defining the optimization step
#The constraint matrix
m <- t(c(0, 0, 0, -1, 1)) <- matrix(c(0, 0, 0, -1, 1), nrow = 5, byrow = T)
##Defining the optimization step
#The constraint matrix
m <- matrix(c(0, 0, 0, -1, 1), nrow = 5, byrow = T)
m
b <- rep(0, 5)
LK <- constrOptim(c(1, 2, 3, 4, 10), f, control = list(fnscale = -1), ui = m, ci = b ) #the optimization step
##Defining the optimization step
#The constraint matrix
m <- matrix(c(0, 0, 0, -1, 1), nrow = 1, byrow = T)
b <- rep(0, 5)
LK <- constrOptim(c(1, 2, 3, 10, 4), f, control = list(fnscale = -1), ui = m, ci = b ) #the optimization step
m
LK <- constrOptim(c(1, 2, 3, 4, 10), f, control = list(fnscale = -1), ui = m, ci = b ) #the optimization step
LK <- constrOptim(c(1, 2, 3, 4, 10), f, control = list(fnscale = -1), ui = m, ci = b, grad = NULL ) #the optimization step
warnings()
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.15.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
##least squares estimation###########################################################
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
##log-likelihood estimation#########################################################
#unidistr function
unidistr <- function(X, a, b){
if(a >= b){
stop('a should be less than b')
}else{
Y <- rep(NA, length(X))
df <- data.frame(X,Y)
names(df) <- c('X', 'Y')
df$Y[df$X < a] <- 0
df$Y[df$X > b] <- 0
df$Y[is.na(df$Y) == T] <- 1/(b - a)
}
return(df)
}
#objective function
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- unidistr(df$X, v[4], v[5]) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
LK <- prod(G$Y) #log likelihood term
return(LK)
}#The objective function for the optimization
##Defining the optimization step
#The constraint matrix
m <- matrix(c(0, 0, 0, -1, 1), nrow = 1, byrow = T)
b <- rep(0, 5)
LK <- constrOptim(c(1, 2, 3, 4, 10), f, control = list(fnscale = -1), ui = m, ci = b, grad = NULL ) #the optimization step
val_LLE <- LK$par
f(c(1,2,3,4,5))
warning()
warnings()
LK <- constrOptim(c(1, 2, 3, 4, 10), f, control = list(fnscale = -1), ui = m, ci = b, grad = NULL ) #the optimization step
warnings()
(1/2)^2001
rm(list=ls())
library(StatsChitran) # package available on github
df <- read.table('D:/SynologyDrive/Didos_self/Career/Future/blog_medium/MaxLikelihhod_vs_LeastSquares/data/data_mk_unif_nr_0.15.txt', sep = '\t', header = T)
plot(df$X, df$Y, type = 'b', col=rgb(0,0,1,0.1), pch=19)
##least squares estimation###########################################################
fn <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v
e <- df$Y - y #the error term
TSE <- sum(e^2) #the sum of squared errors
return(TSE)
}#The objective function for the optimization
L <- optim(c(1, 2, 3), fn) #the optimization step
val_LSE <- L$par
##log-likelihood estimation#########################################################
#unidistr function
unidistr <- function(X, a, b){
if(a >= b){
stop('a should be less than b')
}else{
Y <- rep(NA, length(X))
df <- data.frame(X,Y)
names(df) <- c('X', 'Y')
df$Y[df$X < a] <- 0
df$Y[df$X > b] <- 0
df$Y[is.na(df$Y) == T] <- 1/(b - a)
}
return(df)
}
#objective function
f <- function(v){
y <- v[1]*df$X^2 + v[2]*df$X + v[3] #objective function has co-efficient a, b and c, embedded in vector v.
e <- df$Y - y #the error term
#The function "gauss" used in the next line is available with the StatsChitran package on github
G <- unidistr(df$X, v[4], v[5]) #likelihood vector that each corresponding error term belongs to a gaussian with zero mean and a fixed standard deviation
LLE <- sum(log(G$Y)) #log likelihood term
return(LLE)
}#The objective function for the optimization
##Defining the optimization step
#The constraint matrix
m <- matrix(c(0, 0, 0, -1, 1), nrow = 1, byrow = T)
b <- rep(0, 5)
LLE <- constrOptim(c(1, 2, 3, 4, 10), f, control = list(fnscale = -1), ui = m, ci = b, grad = NULL ) #the optimization step
f(c(1,2,3,4,10))
pnorm(-1, lower.tail = F) - pnorm(1, lower.tail = F)
gauss(3, mu=0, sig = 3, probability = T)
pnorm(3, mean = 0, sd = 1, lower.tail = F)
pnorm(3, mean = 0, sd = 3, lower.tail = F)
2*pnorm(3, mean = 0, sd = 3, lower.tail = F)
2*pnorm(3, mean = 0, sd = 1, lower.tail = F)
